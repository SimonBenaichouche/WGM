{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d5ac0-1593-4019-821b-e083044f97c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbenaich/.local/lib/python3.9/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Training on Problem 1/10 ---\n",
      "  Training method: surrogate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n",
      "/home/sbenaich/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/sbenaich/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_0/surrogate_1746000883/checkpoints/surrogate-epoch=398-val_mse=0.027793.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_0/surrogate_1746000883/checkpoints/surrogate-epoch=398-val_mse=0.027793.ckpt\n",
      "/home/sbenaich/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.029274\n",
      "  Training method: wgm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_0/wgm_1746001296/checkpoints/wgm-epoch=456-val_mse=0.027965.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_0/wgm_1746001296/checkpoints/wgm-epoch=456-val_mse=0.027965.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.029005\n",
      "\n",
      "--- Training on Problem 2/10 ---\n",
      "  Training method: surrogate\n",
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_1/surrogate_1746003252/checkpoints/surrogate-epoch=487-val_mse=0.020299.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_1/surrogate_1746003252/checkpoints/surrogate-epoch=487-val_mse=0.020299.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Method: surrogate, Final Test MSE: 0.020562\n",
      "  Training method: wgm\n",
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_1/wgm_1746003661/checkpoints/wgm-epoch=490-val_mse=0.019808.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_1/wgm_1746003661/checkpoints/wgm-epoch=490-val_mse=0.019808.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.020158\n",
      "\n",
      "--- Training on Problem 3/10 ---\n",
      "  Training method: surrogate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_2/surrogate_1746005618/checkpoints/surrogate-epoch=478-val_mse=0.024289.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_2/surrogate_1746005618/checkpoints/surrogate-epoch=478-val_mse=0.024289.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.024770\n",
      "  Training method: wgm\n",
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_2/wgm_1746006027/checkpoints/wgm-epoch=443-val_mse=0.024100.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_2/wgm_1746006027/checkpoints/wgm-epoch=443-val_mse=0.024100.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.024501\n",
      "\n",
      "--- Training on Problem 4/10 ---\n",
      "  Training method: surrogate\n",
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_3/surrogate_1746007984/checkpoints/surrogate-epoch=476-val_mse=0.019549.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_3/surrogate_1746007984/checkpoints/surrogate-epoch=476-val_mse=0.019549.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.019740\n",
      "  Training method: wgm\n",
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_3/wgm_1746008393/checkpoints/wgm-epoch=489-val_mse=0.019046.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_3/wgm_1746008393/checkpoints/wgm-epoch=489-val_mse=0.019046.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.019271\n",
      "\n",
      "--- Training on Problem 5/10 ---\n",
      "  Training method: surrogate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_4/surrogate_1746010350/checkpoints/surrogate-epoch=482-val_mse=0.023146.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_4/surrogate_1746010350/checkpoints/surrogate-epoch=482-val_mse=0.023146.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.024906\n",
      "  Training method: wgm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_4/wgm_1746010760/checkpoints/wgm-epoch=495-val_mse=0.022858.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_4/wgm_1746010760/checkpoints/wgm-epoch=495-val_mse=0.022858.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.024444\n",
      "\n",
      "--- Training on Problem 6/10 ---\n",
      "  Training method: surrogate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_5/surrogate_1746012716/checkpoints/surrogate-epoch=464-val_mse=0.026231.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_5/surrogate_1746012716/checkpoints/surrogate-epoch=464-val_mse=0.026231.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.026997\n",
      "  Training method: wgm\n",
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_5/wgm_1746013125/checkpoints/wgm-epoch=445-val_mse=0.026633.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_5/wgm_1746013125/checkpoints/wgm-epoch=445-val_mse=0.026633.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.027396\n",
      "\n",
      "--- Training on Problem 7/10 ---\n",
      "  Training method: surrogate\n",
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_6/surrogate_1746015082/checkpoints/surrogate-epoch=491-val_mse=0.017740.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_6/surrogate_1746015082/checkpoints/surrogate-epoch=491-val_mse=0.017740.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.018175\n",
      "  Training method: wgm\n",
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_6/wgm_1746015491/checkpoints/wgm-epoch=468-val_mse=0.017504.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_6/wgm_1746015491/checkpoints/wgm-epoch=468-val_mse=0.017504.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.017955\n",
      "\n",
      "--- Training on Problem 8/10 ---\n",
      "  Training method: surrogate\n",
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_7/surrogate_1746017448/checkpoints/surrogate-epoch=494-val_mse=0.020531.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_7/surrogate_1746017448/checkpoints/surrogate-epoch=494-val_mse=0.020531.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.020798\n",
      "  Training method: wgm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for wgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_7/wgm_1746017857/checkpoints/wgm-epoch=433-val_mse=0.020858.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_7/wgm_1746017857/checkpoints/wgm-epoch=433-val_mse=0.020858.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for wgm.\n",
      "    Starting testing for wgm...\n",
      "    Method: wgm, Final Test MSE: 0.020954\n",
      "\n",
      "--- Training on Problem 9/10 ---\n",
      "  Training method: surrogate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for surrogate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "Restoring states from the checkpoint path at lightning_logs/problem_8/surrogate_1746019812/checkpoints/surrogate-epoch=485-val_mse=0.021170.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs/problem_8/surrogate_1746019812/checkpoints/surrogate-epoch=485-val_mse=0.021170.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | target_mlp    | GradientMLP  | 48.8 K\n",
      "1 | estimator_mlp | GradientMLP2 | 261 K \n",
      "-----------------------------------------------\n",
      "261 K     Trainable params\n",
      "48.8 K    Non-trainable params\n",
      "310 K     Total params\n",
      "1.243     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Finished training for surrogate.\n",
      "    Starting testing for surrogate...\n",
      "    Method: surrogate, Final Test MSE: 0.021557\n",
      "  Training method: wgm\n",
      "    Starting training for wgm...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, IterableDataset, TensorDataset\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "N_PROBLEMS = 10\n",
    "INPUT_DIM = 250\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 4\n",
    "TRAIN_STEPS_PER_EPOCH = 100\n",
    "VAL_STEPS_PER_EPOCH = 1\n",
    "N_TEST_SAMPLES = 2048\n",
    "BATCH_SIZE = 4096\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_EPOCHS = 500\n",
    "\n",
    "WGM_NOISE_STD = 1\n",
    "RFD_EPSILON = 0.0001\n",
    "WGM_LAMBDA = 1.0/np.sqrt(1.0*(INPUT_DIM))\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "# ...\n",
    "\n",
    "# --- Callback pour Historique ---\n",
    "class HistoryCallback(Callback):\n",
    "    \"\"\"Callback pour enregistrer l'historique des métriques de validation.\"\"\"\n",
    "    def __init__(self, monitor='val_mse'):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.history = [] # Stockera les valeurs de la métrique pour un run\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Vérifier si la métrique est présente dans les logs du trainer\n",
    "        # Les logs sont typiquement disponibles après le calcul des métriques de validation\n",
    "        logs = trainer.callback_metrics\n",
    "        if self.monitor in logs:\n",
    "            # Ajouter la valeur actuelle à l'historique de ce run\n",
    "            self.history.append(logs[self.monitor].item())\n",
    "            # print(f\"Epoch {trainer.current_epoch}: Logged {self.monitor}={logs[self.monitor]:.6f}\") # Debug\n",
    "        # else:\n",
    "            # print(f\"Epoch {trainer.current_epoch}: Metric {self.monitor} not found in logs\") # Debug\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Retourne l'historique collecté.\"\"\"\n",
    "        return self.history\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialise l'historique pour un nouveau run.\"\"\"\n",
    "        self.history = []\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    \"\"\"\n",
    "    Activation sinus : y = sin(w0 * x).\n",
    "    w0=30 pour la 1ère couche si on veut une forte oscillation.\n",
    "    \"\"\"\n",
    "    def __init__(self, w0=30.0):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "def siren_init(linear, fan_in, w0=30.0, is_first_layer=False):\n",
    "    \"\"\"\n",
    "    Initialisation SIREN:\n",
    "      - si is_first_layer=True => uniform(-1/fan_in, 1/fan_in)\n",
    "      - sinon => uniform(-sqrt(6/fan_in)/w0, sqrt(6/fan_in)/w0)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        if is_first_layer:\n",
    "            bound = 0.3 #/ fan_in\n",
    "            linear.weight.normal_(mean=0.0, std=bound)\n",
    "        else:\n",
    "            bound = 0.3#/ w0 #math.sqrt(6.0 / fan_in) / w0\n",
    "            linear.weight.normal_(mean=0.0, std=bound)\n",
    "\n",
    "\n",
    "class GradientMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP type SIREN pour générer une fonction J(x) oscillante,\n",
    "    tout en forçant la création du graphe quand on appelle gradient(x).\n",
    "    \n",
    "    On garde la même signature que dans votre code original :\n",
    "      - input_dim, n_layers, hidden_dim, envelope=False\n",
    "    n_layers => nb de couches cachées (hors couche finale).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, n_layers, hidden_dim, envelope=False,\n",
    "                 w0=0.5, w0_hidden=0.5):\n",
    "        super().__init__()\n",
    "        self.envelope = envelope\n",
    "        self.input_dim = input_dim\n",
    "        layers = []\n",
    "        # (1) Première couche\n",
    "        first_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        siren_init(first_linear, fan_in=input_dim, w0=w0, is_first_layer=True)\n",
    "        layers.append(first_linear)\n",
    "        layers.append(Sine(w0=w0))\n",
    "\n",
    "        # (2) Couches cachées\n",
    "        for _ in range(n_layers - 1):\n",
    "            hidden_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "            siren_init(hidden_linear, fan_in=hidden_dim, w0=w0_hidden, is_first_layer=False)\n",
    "            layers.append(hidden_linear)\n",
    "            layers.append(Sine(w0=w0_hidden))\n",
    "\n",
    "        # (3) Couche finale (pas de sinus)\n",
    "        final_linear = nn.Linear(hidden_dim, 1)\n",
    "        bound = 0.3\n",
    "        final_linear.weight.data.normal_(mean=0.0, std=bound)\n",
    "        layers.append(final_linear)\n",
    "        layers.append(Sine(w0=w0_hidden))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sortie scalaire\n",
    "        scalar_output = self.net(x)\n",
    "        # Enveloppe optionnelle\n",
    "        if self.envelope:\n",
    "            #norm = x.pow(4).mean(dim=1, keepdim=True)\n",
    "            norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()  # ||x||\n",
    "        # Centrer la cloche autour de la norme typique\n",
    "            r = 1*WGM_NOISE_STD * np.sqrt(self.input_dim) \n",
    "\n",
    "            # Ajuster ce facteur pour contrôler la décroissance\n",
    "            #r=1#/np.sqrt(self.input_dim)\n",
    "            #r = np.sqrt(self.input_dim)\n",
    "            alpha = torch.clamp(norm-r,min=0)/np.sqrt(self.input_dim) \n",
    "            envelope = torch.exp(- alpha)\n",
    "            scalar_output = scalar_output * envelope\n",
    "        return scalar_output +  0*torch.randn_like(scalar_output)*scalar_output.std()*1e-1\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"\n",
    "        Calcule le gradient wrt x, en forçant l'activation de l'autograd\n",
    "        même si Lightning est en no_grad() pendant la validation ou le test.\n",
    "        \"\"\"\n",
    "        # On override le contexte no_grad() éventuel :\n",
    "        with torch.enable_grad():\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            y_sum = self.forward(x).sum()\n",
    "            # create_graph=True si on veut autoriser des dérivées plus haut ordre\n",
    "            grad = torch.autograd.grad(y_sum, x, create_graph=True)[0]\n",
    "            \n",
    "        return grad \n",
    "\n",
    "class GradientMLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP type SIREN pour générer une fonction J(x) oscillante,\n",
    "    tout en forçant la création du graphe quand on appelle gradient(x).\n",
    "    \n",
    "    On garde la même signature que dans votre code original :\n",
    "      - input_dim, n_layers, hidden_dim, envelope=False\n",
    "    n_layers => nb de couches cachées (hors couche finale).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, n_layers, hidden_dim, envelope=False,\n",
    "                 w0=1.0, w0_hidden=0.5):\n",
    "        super().__init__()\n",
    "        self.envelope = envelope\n",
    "\n",
    "        layers = []\n",
    "        # (1) Première couche\n",
    "        first_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        layers.append(first_linear)\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        # (2) Couches cachées\n",
    "        for _ in range(n_layers - 1):\n",
    "            hidden_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "            layers.append(hidden_linear)\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # (3) Couche finale (pas de sinus)\n",
    "        final_linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        layers.append(final_linear)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sortie scalaire\n",
    "        scalar_output = self.net(x)\n",
    "        # Enveloppe optionnelle\n",
    "        if self.envelope:\n",
    "            norm = x.pow(2).mean(dim=1, keepdim=True)\n",
    "            # Ajuster ce facteur pour contrôler la décroissance\n",
    "            envelope = 1 #torch.exp(-0.5 * norm)\n",
    "            scalar_output = scalar_output * envelope\n",
    "        return scalar_output\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"\n",
    "        Calcule le gradient wrt x, en forçant l'activation de l'autograd\n",
    "        même si Lightning est en no_grad() pendant la validation ou le test.\n",
    "        \"\"\"\n",
    "        # On override le contexte no_grad() éventuel :\n",
    "        with torch.enable_grad():\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            y_sum = self.forward(x).sum()\n",
    "            # create_graph=True si on veut autoriser des dérivées plus haut ordre\n",
    "            grad = torch.autograd.grad(y_sum, x, create_graph=True)[0]\n",
    "        return grad\n",
    "\n",
    "# --- Dataset ---\n",
    "class SampleDataset(IterableDataset):\n",
    "    def __init__(self, input_dim, steps_per_epoch, batch_size, distribution_std=WGM_NOISE_STD):\n",
    "        self.input_dim = input_dim\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.distribution_std = distribution_std\n",
    "        self.step_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.step_count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.step_count < self.steps_per_epoch:\n",
    "            self.step_count += 1\n",
    "            x = torch.randn(self.batch_size, self.input_dim, device=device) * self.distribution_std\n",
    "            return torch.cat([x,x,x,x,x])\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "# --- Lightning Module ---\n",
    "class GradientEstimationExperiment(pl.LightningModule):\n",
    "    def __init__(self, target_mlp, estimator_mlp, method, lr, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.target_mlp = target_mlp\n",
    "        self.estimator_mlp = estimator_mlp\n",
    "        self.method = method\n",
    "        self.p_std_sq = WGM_NOISE_STD ** 2\n",
    "        self.rfd_epsilon = hparams.get(\"rfd_epsilon\", 1e-4)\n",
    "        self.wgm_n_samples = 1\n",
    "        self.val_historic = []\n",
    "        self.l2_historic = []\n",
    "        self.grad_l2_historic = []\n",
    "        \n",
    "        for p in self.target_mlp.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.target_mlp.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.estimator_mlp.gradient(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        loss = None\n",
    "\n",
    "        if self.method == 'direct_mse':\n",
    "            grad_j_true = self.target_mlp.gradient(x.detach())\n",
    "            s_theta = self.estimator_mlp.gradient(x)\n",
    "            loss = F.mse_loss(s_theta, grad_j_true)\n",
    "\n",
    "        elif self.method == 'wgm':\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            with torch.no_grad():\n",
    "                j_val = self.target_mlp(x).squeeze()\n",
    "        \n",
    "            s_output = self.estimator_mlp(x)  # scalaire\n",
    "            s_theta = torch.autograd.grad(s_output.sum(), x, create_graph=True)[0]\n",
    "        \n",
    "            div_s_approx = 0.0\n",
    "            for _ in range(self.wgm_n_samples):\n",
    "                #v = torch.randint_like(x, low=0, high=2).float() * 2.0 - 1.0\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "                v = torch.randn_like(x)\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8) \n",
    "                \n",
    "                s_dot_v = (s_theta * v).sum(dim=1)\n",
    "                grad_s_dot_v = torch.autograd.grad(s_dot_v.sum(), x, create_graph=True, allow_unused=True)[0]\n",
    "                div_s_approx += (grad_s_dot_v * v).sum(dim=1)\n",
    "        \n",
    "            div_s_approx /= (1.0*self.wgm_n_samples)\n",
    "            grad_log_p = -x / self.p_std_sq\n",
    "        \n",
    "            term1 = (s_theta ** 2).sum(dim=1)\n",
    "            term2 = 2 * j_val * (div_s_approx + (s_theta * grad_log_p).sum(dim=1))\n",
    "            loss = (term1 + 1 * term2).mean()\n",
    "    \n",
    "\n",
    "        elif self.method == 'rfd_mse':\n",
    "            with torch.no_grad():\n",
    "                v = torch.randn_like(x)\n",
    "                v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "                j_plus = self.target_mlp(x + self.rfd_epsilon * v).squeeze()\n",
    "                j_minus = self.target_mlp(x - self.rfd_epsilon * v).squeeze()\n",
    "                fd_dir_deriv = (j_plus - j_minus) / (2 * self.rfd_epsilon)\n",
    "\n",
    "            s_theta_dot_v = (self.estimator_mlp.gradient(x) * v).sum(dim=1)\n",
    "            loss = F.mse_loss(s_theta_dot_v, fd_dir_deriv)\n",
    "\n",
    "\n",
    "        \n",
    "        elif self.method == 'surrogate':\n",
    "\n",
    "            j = self.target_mlp(x).squeeze()\n",
    "            j_s_theta = self.estimator_mlp(x).sum(dim=1)\n",
    "        \n",
    "            loss = F.mse_loss(j, j_s_theta)\n",
    "\n",
    "        elif self.method == \"mixed\":\n",
    "            \n",
    "            j = self.target_mlp(x).squeeze()\n",
    "            j_s_theta = self.estimator_mlp(x).sum(dim=1)\n",
    "        \n",
    "            loss = F.mse_loss(j, j_s_theta)\n",
    "\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            with torch.no_grad():\n",
    "                j_val = self.target_mlp(x).squeeze()\n",
    "        \n",
    "            s_output = self.estimator_mlp(x)  # scalaire\n",
    "            s_theta = torch.autograd.grad(s_output.sum(), x, create_graph=True)[0]\n",
    "        \n",
    "            div_s_approx = 0.0\n",
    "            for _ in range(self.wgm_n_samples):\n",
    "                v = torch.randint_like(x, low=0, high=2).float() * 2.0 - 1.0\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "                #v = torch.randn_like(x)\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8) \n",
    "                \n",
    "                s_dot_v = (s_theta * v).sum(dim=1)\n",
    "                grad_s_dot_v = torch.autograd.grad(s_dot_v.sum(), x, create_graph=True, allow_unused=True)[0]\n",
    "                div_s_approx += (grad_s_dot_v * v).sum(dim=1)\n",
    "        \n",
    "            div_s_approx /= (1.0*self.wgm_n_samples)\n",
    "            grad_log_p = -x / self.p_std_sq\n",
    "        \n",
    "            term1 = (s_theta ** 2).sum(dim=1)\n",
    "            term2 = 2 * j_val * (div_s_approx + (s_theta * grad_log_p).sum(dim=1))\n",
    "            loss += (term1 + 1 * term2).mean()\n",
    "            loss *= 0.5\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        grad_j_true = self.target_mlp.gradient(x.detach())\n",
    "        j_true = self.target_mlp(x.detach()).sum(dim=1).detach()\n",
    "        \n",
    "        self.grad_l2_historic.append((grad_j_true**2).mean(dim=1).mean().detach())\n",
    "        self.l2_historic.append((j_true**2).mean().detach())\n",
    "\n",
    "        s_theta = self.estimator_mlp.gradient(x)\n",
    "        val_mse = F.mse_loss(s_theta, grad_j_true)\n",
    "        self.log('val_mse', val_mse, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.val_historic.append(val_mse.cpu().detach())\n",
    "        return val_mse\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        grad_j_true = self.target_mlp.gradient(x.detach())\n",
    "        s_theta = self.estimator_mlp.gradient(x)\n",
    "        test_mse = F.mse_loss(s_theta, grad_j_true)\n",
    "        self.log('test_mse', test_mse, on_step=False, on_epoch=True)\n",
    "        return test_mse\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure l'optimiseur Adam et un scheduler ExponentialLR simple.\"\"\"\n",
    "\n",
    "        # Créer l'optimiseur Adam\n",
    "        # Utilise le learning rate défini dans les hyperparamètres (accessible via self.hparams)\n",
    "        optimizer = optim.Adam(self.estimator_mlp.parameters(), lr=self.hparams['lr'])\n",
    "\n",
    "        # Créer un scheduler de learning rate (exemple: ExponentialLR)\n",
    "        # Réduit le LR de 2% à chaque époque (gamma=0.98)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.998)\n",
    "\n",
    "        # Retourner l'optimiseur et le scheduler configuré pour Pytorch Lightning\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",  # Mettre à jour le LR à chaque époque\n",
    "                \"frequency\": 1,       # Mettre à jour à chaque intervalle (chaque époque ici)\n",
    "                # 'monitor': 'val_loss' # Optionnel, utile seulement pour certains schedulers comme ReduceLROnPlateau\n",
    "            },\n",
    "        }\n",
    "\n",
    "# --- Training Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = { 'wgm': [],'surrogate': []}\n",
    "    trained_estimators = { 'wgm': [],'surrogate': []}\n",
    "    val_loss_per_epoch = { 'wgm': [],'surrogate': []}\n",
    "    grad_l2_historic_epoch = { 'wgm': [],'surrogate': []}\n",
    "    l2_historic_epoch = { 'wgm': [],'surrogate': []}\n",
    "\n",
    "    '''\n",
    "    all_results = { 'mixed': [],'surrogate': []}\n",
    "    trained_estimators = { 'mixed': [],'surrogate': []}\n",
    "    val_loss_per_epoch = { 'mixed': [],'surrogate': []}\n",
    "    grad_l2_historic_epoch = { 'mixed': [],'surrogate': []}\n",
    "    l2_historic_epoch = { 'mixed': [],'surrogate': []}'''\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range(N_PROBLEMS):\n",
    "        print(f\"\\n--- Training on Problem {i+1}/{N_PROBLEMS} ---\")\n",
    "\n",
    "        target_mlp = GradientMLP(INPUT_DIM, 2, 128, envelope=True).to(device)\n",
    "\n",
    "        train_dataset = SampleDataset(INPUT_DIM, TRAIN_STEPS_PER_EPOCH, BATCH_SIZE)\n",
    "        val_dataset   = SampleDataset(INPUT_DIM, VAL_STEPS_PER_EPOCH, BATCH_SIZE)\n",
    "        X_test = torch.randn(N_TEST_SAMPLES, INPUT_DIM, device=device) * WGM_NOISE_STD\n",
    "        test_dataset = TensorDataset(X_test)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=None, num_workers=0)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=None, num_workers=0)\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "        for method in ['surrogate', 'wgm']:\n",
    "            print(f\"  Training method: {method}\")\n",
    "\n",
    "            estimator_mlp = GradientMLP2(INPUT_DIM, N_LAYERS, HIDDEN_DIM, envelope=False).to(device)\n",
    "\n",
    "            hparams_dict = {\n",
    "                \"input_dim\": INPUT_DIM, \"hidden_dim\": HIDDEN_DIM, \"n_layers\": N_LAYERS,\n",
    "                \"lr\": LEARNING_RATE, \"method\": method, \"wgm_lambda\": WGM_LAMBDA,\n",
    "                \"rfd_epsilon\": RFD_EPSILON, \"problem_idx\": i\n",
    "            }\n",
    "\n",
    "            experiment = GradientEstimationExperiment(target_mlp, estimator_mlp, method, LEARNING_RATE, hparams_dict)\n",
    "            logger = pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"problem_{i}\", version=f\"{method}_{int(time.time())}\")\n",
    "\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=MAX_EPOCHS,\n",
    "                accelerator=\"auto\",\n",
    "                devices=1,\n",
    "                logger=logger,\n",
    "                callbacks=[\n",
    "                    pl.callbacks.ModelCheckpoint(\n",
    "                        monitor=\"val_mse\", mode=\"min\",\n",
    "                        filename=f\"{method}-{{epoch:02d}}-{{val_mse:.6f}}\"\n",
    "                    )\n",
    "                ],\n",
    "                enable_progress_bar=False,\n",
    "                enable_model_summary=True,\n",
    "                num_sanity_val_steps=0,\n",
    "                limit_train_batches=TRAIN_STEPS_PER_EPOCH,\n",
    "                limit_val_batches=VAL_STEPS_PER_EPOCH,\n",
    "                inference_mode=False,\n",
    "            )\n",
    "\n",
    "            print(f\"    Starting training for {method}...\")\n",
    "            trainer.fit(experiment, train_loader, val_loader)\n",
    "            print(f\"    Finished training for {method}.\")\n",
    "\n",
    "            print(f\"    Starting testing for {method}...\")\n",
    "            test_results = trainer.test(ckpt_path=\"best\", dataloaders=test_loader, verbose=False)\n",
    "            if test_results:\n",
    "                final_mse = test_results[0].get('test_mse')\n",
    "                if final_mse is not None:\n",
    "                    all_results[method].append(final_mse)\n",
    "                    print(f\"    Method: {method}, Final Test MSE: {final_mse:.6f}\")\n",
    "                else:\n",
    "                    print(f\"    Method: {method}, 'test_mse' not found.\")\n",
    "                    all_results[method].append(float('nan'))\n",
    "            else:\n",
    "                print(f\"    Method: {method}, Testing failed.\")\n",
    "                all_results[method].append(float('nan'))\n",
    "\n",
    "            # ✅ Stocker le modèle estimé pour visualisation future\n",
    "            trained_estimators[method].append(experiment.estimator_mlp.cpu().eval())\n",
    "            val_loss_per_epoch[method].append(experiment.val_historic)\n",
    "            grad_l2_historic_epoch[method].append(experiment.grad_l2_historic)\n",
    "            l2_historic_epoch[method].append(experiment.l2_historic)\n",
    "\n",
    "    print(\"\\n--- Average Results ---\")\n",
    "    for method in [ 'mixed',  'surrogate']:\n",
    "        results = [res for res in all_results[method] if not np.isnan(res)]\n",
    "        if results:\n",
    "            avg_mse = np.mean(results)\n",
    "            std_mse = np.std(results)\n",
    "            print(f\"Method: {method:<12} | Avg Test MSE: {avg_mse:.6f} +/- {std_mse:.6f}  ({len(results)} successful runs)\")\n",
    "        else:\n",
    "            print(f\"Method: {method:<12} | No successful runs found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cee11-8663-417f-85dd-904ddcb2145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- la fonction d’aide que tu avais déjà ---\n",
    "def extract_ts(method, results):\n",
    "    runs = results[method]                # list[list[float]]\n",
    "    arr = torch.zeros((len(runs), len(runs[0])))\n",
    "    for i, run in enumerate(runs):\n",
    "        arr[i] = torch.tensor(run)\n",
    "    return arr.cpu().numpy()              # shape = (n_runs, n_epochs)\n",
    "\n",
    "# --- on récupère les séries ---\n",
    "#ts_wgm       = extract_ts(\"wgm\",       val_loss_per_epoch)\n",
    "ts_wgm       = extract_ts(\"wgm\",       val_loss_per_epoch)\n",
    "\n",
    "ts_surrogate = extract_ts(\"surrogate\", val_loss_per_epoch)\n",
    "\n",
    "l2_surrogate = extract_ts(\"wgm\",       l2_historic_epoch)\n",
    "l2_wgm = extract_ts(\"surrogate\", l2_historic_epoch)\n",
    "\n",
    "grad_l2_surrogate = extract_ts(\"wgm\",       grad_l2_historic_epoch)\n",
    "grad_l2_wgm = extract_ts(\"surrogate\", grad_l2_historic_epoch)\n",
    "\n",
    "import os   \n",
    "os.makedirs(\"metrics\", exist_ok=True)\n",
    "\n",
    "# --- variante NumPy (.npy) : -----\n",
    "np.save(\"metrics/ts_wgm.npy\",             ts_wgm)\n",
    "np.save(\"metrics/ts_surrogate.npy\",       ts_surrogate)\n",
    "np.save(\"metrics/l2_surrogate.npy\",       l2_surrogate)\n",
    "np.save(\"metrics/l2_wgm.npy\",             l2_wgm)\n",
    "np.save(\"metrics/grad_l2_surrogate.npy\",  grad_l2_surrogate)\n",
    "np.save(\"metrics/grad_l2_wgm.npy\",        grad_l2_wgm)\n",
    "\n",
    "# --- moyenne & écart-type sur les runs (axe 0) ---\n",
    "mean_wgm, std_wgm             = ts_wgm.mean(0), ts_wgm.std(0)\n",
    "mean_surrogate, std_surrogate = ts_surrogate.mean(0), ts_surrogate.std(0)\n",
    "\n",
    "epochs = np.arange(len(mean_wgm))  # X-axis\n",
    "\n",
    "# --- figure ---\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(epochs, mean_wgm,       label=\"WGM\")\n",
    "plt.fill_between(epochs,\n",
    "                 mean_wgm - std_wgm,\n",
    "                 mean_wgm + std_wgm,\n",
    "                 alpha=0.2)\n",
    "\n",
    "plt.plot(epochs, mean_surrogate, label=\"Surrogate\")\n",
    "plt.fill_between(epochs,\n",
    "                 mean_surrogate - std_surrogate,\n",
    "                 mean_surrogate + std_surrogate,\n",
    "                 alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "#plt.yscale(\"log\")              # retire si tu préfères l’échelle linéaire\n",
    "plt.legend()\n",
    "plt.title(\"Évolution moyenne de la loss (±1 σ)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f326de1-699c-4e8c-979e-f2ba47f0ea21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343a458-3e6e-4184-aaf9-7dc49087351c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4b809-9619-4da9-9962-8ad5eb246b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def plot_scalar_surfaces(target_mlp, scalar_estimators, methods, fixed_vars=None, resolution=200, range_min=-6, range_max=6):\n",
    "    \"\"\"\n",
    "    Affiche les graphes de niveau (contours) de la fonction scalaire cible et des estimateurs pour une analyse qualitative.\n",
    "\n",
    "    Args:\n",
    "        target_mlp (nn.Module): Le modèle cible (retourne un scalaire).\n",
    "        scalar_estimators (dict): Dictionnaire de modèles entraînés {method_name: estimator_model}.\n",
    "        methods (list): Liste des noms de méthodes à afficher.\n",
    "        fixed_vars (list or tensor): Valeurs des dimensions fixes (INPUT_DIM - 2).\n",
    "        resolution (int): Résolution de la grille (pixels).\n",
    "        range_min (float): Min des axes x/y.\n",
    "        range_max (float): Max des axes x/y.\n",
    "    \"\"\"\n",
    "    INPUT_DIM = target_mlp.net[0].in_features  # suppose GradientMLP\n",
    "    assert INPUT_DIM >= 2, \"Input dimension must be at least 2 for plotting.\"\n",
    "\n",
    "    device = next(target_mlp.parameters()).device\n",
    "\n",
    "    if fixed_vars is None:\n",
    "        fixed_vars = torch.zeros(INPUT_DIM - 2, device=device)+1\n",
    "    else:\n",
    "        fixed_vars = torch.tensor(fixed_vars, dtype=torch.float32, device=device)\n",
    "\n",
    "    x = np.linspace(range_min, range_max, resolution)\n",
    "    y = np.linspace(range_min, range_max, resolution)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    grid_points = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n",
    "\n",
    "    fixed_exp = fixed_vars.unsqueeze(0).expand(resolution ** 2, -1)\n",
    "    grid_tensor = torch.tensor(grid_points, dtype=torch.float32, device=device)\n",
    "    full_inputs = torch.cat([grid_tensor, fixed_exp], dim=1)\n",
    "\n",
    "    # Ground Truth\n",
    "    with torch.no_grad():\n",
    "        gt_values = target_mlp(full_inputs).view(resolution, resolution).cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(5 * (len(methods) + 1), 4))\n",
    "    plt.subplot(1, len(methods)+1, 1)\n",
    "    plt.contourf(xx, yy, gt_values, levels=50, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title('Ground Truth')\n",
    "\n",
    "    for idx, method in enumerate(methods):\n",
    "        model = scalar_estimators[method]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            est_values = model(full_inputs).view(resolution, resolution).cpu().numpy()\n",
    "\n",
    "        plt.subplot(1, len(methods)+1, idx+2)\n",
    "        plt.contourf(xx, yy, est_values, levels=50, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title(f'{method.upper()} Estimate')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be57ed-6f13-4f0b-b533-8e11a982297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scalar_estimators_dict = {\n",
    "    #method: trained_estimators[method][-1] for method in ['direct_mse', 'wgm', 'rfd_mse','surrogate','mixed']\n",
    "        method: trained_estimators[method][-1] for method in ['wgm', 'surrogate']\n",
    "\n",
    "}\n",
    "\n",
    "fixed_vars = [-0.20] * (INPUT_DIM - 2)\n",
    "plot_scalar_surfaces(target_mlp.cpu().eval(), scalar_estimators_dict, list(scalar_estimators_dict.keys()), fixed_vars=fixed_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d478f20-b438-462e-9d85-36bdd5e2ea3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43adf4f-4473-4b85-b78b-4283dba87906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecab1d6-1889-4590-80fc-9d0382d48e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0f909-276c-4d7b-9cb9-896602dfc2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7f321-d945-4209-9088-e91759c6986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['direct_mse', 'wgm', 'rfd_mse', 'surrogate', 'mixed']:\n",
    "    results = [res for res in all_results[method] if not np.isnan(res)]\n",
    "    if results:\n",
    "        avg_mse = np.mean(results)\n",
    "        std_mse = np.std(results)\n",
    "        print(f\"Method: {method:<12} | Avg Test MSE: {avg_mse:.6f} +/- {std_mse:.6f}  ({len(results)} successful runs)\")\n",
    "    else:\n",
    "        print(f\"Method: {method:<12} | No successful runs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ace425-c118-4f6e-a18f-a279c833e65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18365ca-a112-42d9-8dee-0a61c84c51c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ab34e-9b5b-4086-b8af-481035b36012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b357c0-4832-4ac4-b2d9-6ac75bba317c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
