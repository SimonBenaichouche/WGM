{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace77fac-edaf-4530-b358-ed65f858ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "try:\n",
    "  import pytorch_lightning as pl\n",
    "except:\n",
    "  !pip install pytorch-lightning\n",
    "  import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader, IterableDataset, TensorDataset\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "# NOTE: No Matplotlib imports here yet\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# --- Configuration ---\n",
    "N_PROBLEMS = 3             # Set to 1 for simplicity in saving/loading models later\n",
    "INPUT_DIM = 2\n",
    "HIDDEN_DIM = 128\n",
    "N_LAYERS = 4\n",
    "TRAIN_STEPS_PER_EPOCH = 300\n",
    "VAL_STEPS_PER_EPOCH = 1  # Increased slightly for more stable validation\n",
    "N_TEST_SAMPLES = 1024\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_EPOCHS = 400 # Reduced for faster execution example, increase for better results\n",
    "\n",
    "WGM_NOISE_STD = 2.0\n",
    "RFD_EPSILON = 0.01\n",
    "WGM_LAMBDA = 1.0  # Often 1/sigma^2\n",
    "WGM_N_SAMPLES = 1\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Components ---\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=30.0):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "# siren_init function is defined but not used by GradientMLP2 currently\n",
    "def siren_init(linear, fan_in, w0=30.0, is_first_layer=False):\n",
    "    with torch.no_grad():\n",
    "        if is_first_layer:\n",
    "            bound = 1.0 / fan_in\n",
    "            linear.weight.uniform_(-bound, bound)\n",
    "        else:\n",
    "            bound = math.sqrt(6.0 / fan_in) / w0\n",
    "            linear.weight.uniform_(-bound, bound)\n",
    "        if linear.bias is not None:\n",
    "             linear.bias.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "class AnalyticalTarget(nn.Module):\n",
    "    r\"\"\"\n",
    "    Fonctions analytiques usuelles pour benchmarks de régression / optimisation.\n",
    "\n",
    "    mode ∈ {\n",
    "        \"rosenbrock\",          # d ≥ 2\n",
    "        \"ackley\",              # d ≥ 2\n",
    "        \"styblinski\",          # d ≥ 1\n",
    "        \"booth\",               # d = 2\n",
    "        \"mccormick\"            # d = 2\n",
    "    }\n",
    "    Toutes les sorties sont multipliées par `scale` (par défaut 0.1) afin de\n",
    "    rester dans une amplitude raisonnable pour l'apprentissage.\n",
    "    \"\"\"\n",
    "    def __init__(self, mode=\"rosenbrock\", dim=2,\n",
    "                 a=1.0, b=100.0, scale=1, radius_d = 20,alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.mode  = mode.lower()\n",
    "        self.dim   = dim\n",
    "        self.a, self.b = a, b\n",
    "        self.scale = scale\n",
    "        self.radius_d = radius_d\n",
    "        self.alpha = alpha\n",
    "        valid = {\"rosenbrock\", \"ackley\",\n",
    "                 \"styblinski\", \"booth\", \"mccormick\"}\n",
    "        if self.mode not in valid:\n",
    "            raise ValueError(f\"Unknown mode {self.mode}\")\n",
    "        if self.mode in {\"booth\", \"mccormick\"} and dim != 2:\n",
    "            raise ValueError(f\"{self.mode} defined only for dim=2, got dim={dim}\")\n",
    "        if dim < 1:\n",
    "            raise ValueError(\"dim must be ≥ 1\")\n",
    "\n",
    "    # -------------------------------------------------- #\n",
    "    def forward(self, x):                        # x : (B, dim)\n",
    "        x = x.to(x.device)                       # safety\n",
    "        if self.mode == \"rosenbrock\":\n",
    "            y = 0.0\n",
    "            for i in range(self.dim - 1):\n",
    "                xi, xip1 = x[:, i], x[:, i + 1]\n",
    "                y += (self.a - xi) ** 2 + self.b * (xip1 - xi ** 2) ** 2\n",
    "            y = 1e-3*y\n",
    "\n",
    "        elif self.mode == \"ackley\":\n",
    "            part1 = -20.0 * torch.exp(\n",
    "                -0.2 * torch.sqrt(0.5 * (x ** 2).sum(dim=1))\n",
    "            )\n",
    "            part2 = -torch.exp(\n",
    "                torch.cos(2 * math.pi * x).mean(dim=1)\n",
    "            )\n",
    "            y = part1 + part2\n",
    "            y= 1e-1*y\n",
    "\n",
    "        elif self.mode == \"styblinski\":\n",
    "            # f(x) = 0.5 Σ_i (x_i^4 - 16 x_i^2 + 5 x_i)\n",
    "            y = 0.5 * (x ** 4 - 16 * x ** 2 + 5 * x).sum(dim=1)\n",
    "            y = 1e-2*y\n",
    "\n",
    "        elif self.mode == \"booth\":\n",
    "            x1, x2 = x[:, 0], x[:, 1]\n",
    "            y = (x1 + 2 * x2 - 7) ** 2 + (2 * x1 + x2 - 5) ** 2\n",
    "\n",
    "        elif self.mode == \"mccormick\":\n",
    "            x1, x2 = x[:, 0], x[:, 1]\n",
    "            y = torch.sin(x1 + x2) + (x1 - x2) ** 2 - 1.5 * x1 + 2.5 * x2 + 1.0\n",
    "        y = (self.scale * y).unsqueeze(1)  # (B, 1)\n",
    "\n",
    "        # Appliquer la fonction test radiale\n",
    "        radial_mask = torch.norm(x, dim=1)**2  # (B,)\n",
    "        weight = torch.ones_like(radial_mask)\n",
    "        inside = radial_mask <= self.radius_d\n",
    "        weight[~inside] = torch.exp(-self.alpha * (radial_mask[~inside] - self.radius_d))\n",
    "        y = y * weight.unsqueeze(1)  # Appliquer pondération\n",
    "\n",
    "        return y\n",
    "\n",
    "    # -------------------------------------------------- #\n",
    "    def gradient(self, x):\n",
    "        x = x.to(x.device)\n",
    "        with torch.enable_grad():\n",
    "            x_req = x.detach().requires_grad_(True)\n",
    "            y_sum = self.forward(x_req).sum()\n",
    "            grad  = torch.autograd.grad(y_sum, x_req,\n",
    "                                        create_graph=False)[0]\n",
    "        return grad\n",
    "\n",
    "class GradientMLP2(nn.Module):\n",
    "    \"\"\" MLP using Tanh activation. \"\"\"\n",
    "    def __init__(self, input_dim, n_layers, hidden_dim, envelope=False,\n",
    "                 w0=1.0, w0_hidden=0.5):\n",
    "        super().__init__()\n",
    "        self.envelope = envelope\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.GELU())\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.get_device())\n",
    "        scalar_output = self.net(x)\n",
    "        if self.envelope:\n",
    "            norm = x.pow(2).mean(dim=1, keepdim=True)\n",
    "            envelope_val = torch.exp(-0.5 * norm)\n",
    "            scalar_output = scalar_output * envelope_val\n",
    "        return scalar_output\n",
    "\n",
    "    def gradient(self, x):\n",
    "        x = x.to(self.get_device())\n",
    "        with torch.enable_grad():\n",
    "            x_detached = x.detach().requires_grad_(True)\n",
    "            y_sum = self.forward(x_detached).sum()\n",
    "            grad = torch.autograd.grad(y_sum, x_detached, create_graph=True)[0]\n",
    "        return grad\n",
    "\n",
    "    def get_device(self):\n",
    "        try:\n",
    "            return next(self.parameters()).device\n",
    "        except StopIteration:\n",
    "            return device\n",
    "\n",
    "\n",
    "# --- Dataset ---\n",
    "class SampleDataset(IterableDataset):\n",
    "    def __init__(self, input_dim, steps_per_epoch, batch_size, distribution_std=WGM_NOISE_STD):\n",
    "        self.input_dim = input_dim\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.distribution_std = distribution_std\n",
    "        self.step_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.step_count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.step_count < self.steps_per_epoch:\n",
    "            self.step_count += 1\n",
    "            x = torch.randn(self.batch_size, self.input_dim, device=device) * self.distribution_std\n",
    "            return torch.cat([x])\n",
    "        else:\n",
    "            raise StopIteration\n",
    "class TestDataset(TensorDataset):\n",
    "    def __init__(self, n_samples, input_dim=INPUT_DIM, noise_std=WGM_NOISE_STD):\n",
    "        X_test = torch.randn(n_samples, input_dim) * noise_std\n",
    "        super().__init__(X_test.to(device))\n",
    "\n",
    "# --- Lightning Module ---\n",
    "class GradientEstimationExperiment(pl.LightningModule):\n",
    "    def __init__(self, target_mlp, estimator_mlp, method, lr, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams, ignore=['target_mlp', 'estimator_mlp'])\n",
    "        self.target_mlp = target_mlp\n",
    "        self.estimator_mlp = estimator_mlp\n",
    "        self.method = method\n",
    "        self.lr = lr\n",
    "        self.p_std_sq = self.hparams.get(\"wgm_noise_std\", WGM_NOISE_STD) ** 2\n",
    "        self.rfd_epsilon = self.hparams.get(\"rfd_epsilon\", RFD_EPSILON)\n",
    "        self.wgm_n_samples = self.hparams.get(\"wgm_n_samples\", WGM_N_SAMPLES)\n",
    "        self.wgm_lambda = self.hparams.get(\"wgm_lambda\", WGM_LAMBDA)\n",
    "\n",
    "        for p in self.target_mlp.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.target_mlp.eval()\n",
    "\n",
    "        # Internal history (optional, but useful)\n",
    "        self.val_mse_history = []\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.estimator_mlp.gradient(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        if self.method == 'direct_mse':\n",
    "            grad_j_true = self.target_mlp.gradient(x.detach())\n",
    "            s_theta = self.estimator_mlp.gradient(x)\n",
    "            loss = F.mse_loss(s_theta, grad_j_true)\n",
    "\n",
    "        elif self.method == 'wgm':\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            with torch.no_grad():\n",
    "                j_val = self.target_mlp(x).squeeze()\n",
    "\n",
    "            s_output = self.estimator_mlp(x)  # scalaire\n",
    "            s_theta = torch.autograd.grad(s_output.sum(), x, create_graph=True)[0]\n",
    "\n",
    "            div_s_approx = 0.0\n",
    "            for _ in range(2):\n",
    "                #v = torch.randint_like(x, low=0, high=2).float() * 2.0 - 1.0\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "                v = torch.randn_like(x)\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "                '''s_dot_v = (s_theta * v).sum(dim=1)\n",
    "                grad_s_dot_v = torch.autograd.grad(s_dot_v.sum(), x, create_graph=True, allow_unused=True)[0]\n",
    "                div_s_approx += (grad_s_dot_v * v).sum(dim=1)'''\n",
    "\n",
    "                div_s_approx += torch.autograd.grad(s_theta[:,_].sum(), x, create_graph=True, allow_unused=True)[0][:,_]\n",
    "\n",
    "            grad_log_p = -x / self.p_std_sq\n",
    "\n",
    "            term1 = (s_theta ** 2).sum(dim=1)\n",
    "            term2 = 2 * j_val * (div_s_approx + (s_theta * grad_log_p).sum(dim=1))\n",
    "            loss = 1*(term1 + term2).mean()\n",
    "\n",
    "        elif self.method == 'rfd_mse':\n",
    "             with torch.no_grad():\n",
    "                v = torch.randn_like(x)\n",
    "                v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "                x_detached = x.detach()\n",
    "                j_plus = self.target_mlp(x_detached + self.rfd_epsilon * v).squeeze()\n",
    "                j_minus = self.target_mlp(x_detached - self.rfd_epsilon * v).squeeze()\n",
    "                fd_dir_deriv = (j_plus - j_minus) / (2 * self.rfd_epsilon)\n",
    "             s_theta = self.estimator_mlp.gradient(x)\n",
    "             s_theta_dot_v = (s_theta * v).sum(dim=1)\n",
    "             loss = F.mse_loss(s_theta_dot_v, fd_dir_deriv)\n",
    "\n",
    "        elif self.method == 'surrogate':\n",
    "            j_true = self.target_mlp(x.detach()).squeeze()\n",
    "            j_s_theta = self.estimator_mlp(x).squeeze()\n",
    "            loss = F.mse_loss(j_true, j_s_theta)\n",
    "        # Added basic mixed method from original code for completeness if needed\n",
    "        elif self.method == \"mixed\":\n",
    "            # Surrogate part\n",
    "            j_true_surrogate = self.target_mlp(x.detach()).squeeze()\n",
    "            j_s_theta_surrogate = self.estimator_mlp(x).squeeze()\n",
    "            loss_surrogate = F.mse_loss(j_true_surrogate, j_s_theta_surrogate)\n",
    "\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            with torch.no_grad():\n",
    "                j_val = self.target_mlp(x).squeeze()\n",
    "\n",
    "            s_output = self.estimator_mlp(x)  # scalaire\n",
    "            s_theta = torch.autograd.grad(s_output.sum(), x, create_graph=True)[0]\n",
    "\n",
    "            div_s_approx = 0.0\n",
    "            for _ in range(2):\n",
    "                #v = torch.randint_like(x, low=0, high=2).float() * 2.0 - 1.0\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "                v = torch.randn_like(x)\n",
    "                #v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "                '''s_dot_v = (s_theta * v).sum(dim=1)\n",
    "                grad_s_dot_v = torch.autograd.grad(s_dot_v.sum(), x, create_graph=True, allow_unused=True)[0]\n",
    "                div_s_approx += (grad_s_dot_v * v).sum(dim=1)'''\n",
    "\n",
    "                div_s_approx += torch.autograd.grad(s_theta[:,_].sum(), x, create_graph=True, allow_unused=True)[0][:,_]\n",
    "\n",
    "            grad_log_p = -x / self.p_std_sq\n",
    "\n",
    "            term1 = (s_theta ** 2).sum(dim=1)\n",
    "            term2 = 2 * j_val * (div_s_approx + (s_theta * grad_log_p).sum(dim=1))\n",
    "            loss_wgm = (term1 + 1 * term2).mean()\n",
    "\n",
    "            # Combine losses (e.g., average)\n",
    "            loss = 0.75 * loss_surrogate + 0.25 * loss_wgm\n",
    "\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        grad_j_true = self.target_mlp.gradient(x.detach())\n",
    "        s_theta = self.forward(x) # Calls estimator_mlp.gradient(x)\n",
    "        val_mse = F.mse_loss(s_theta, grad_j_true)\n",
    "        self.log('val_mse', val_mse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.val_mse_history.append(val_mse.item()) # Store history\n",
    "        return val_mse\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        grad_j_true = self.target_mlp.gradient(x.detach())\n",
    "        s_theta = self.forward(x)\n",
    "        test_mse = F.mse_loss(s_theta, grad_j_true)\n",
    "        self.log('test_mse', test_mse, on_step=False, on_epoch=True, logger=True)\n",
    "        return test_mse\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.estimator_mlp.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "        return {\"optimizer\": optimizer,\"lr_scheduler\": {\"scheduler\": scheduler,\"interval\": \"epoch\", \"frequency\": 1}}\n",
    "\n",
    "# ================= MAIN TRAINING SCRIPT ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # Define methods to run - use the ones from your original code\n",
    "    methods_to_run = ['wgm', 'surrogate',\"mixed\"] # Or ['mixed', 'surrogate'] as in your comment\n",
    "\n",
    "    all_results = {method: [] for method in methods_to_run}\n",
    "    # THIS IS THE IMPORTANT DICTIONARY TO STORE MODELS FOR PLOTTING LATER\n",
    "    trained_estimators = {method: [] for method in methods_to_run}\n",
    "\n",
    "    # We only run for N_PROBLEMS=1 here, so target is defined once\n",
    "    target = AnalyticalTarget(mode=\"styblinski\").to(device) # Define target once\n",
    "    target = AnalyticalTarget(mode=\"rosenbrock\").to(device) # Define target once\n",
    "\n",
    "\n",
    "    for i in range(N_PROBLEMS): # Loop kept for structure, but N_PROBLEMS=1\n",
    "        print(f\"\\n{'='*10} Training Problem {i+1}/{N_PROBLEMS} {'='*10}\")\n",
    "\n",
    "        train_x = torch.randn(8 * BATCH_SIZE, INPUT_DIM) * WGM_NOISE_STD\n",
    "        train_ds = TensorDataset(train_x.to(device))\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "        val_dataset   = SampleDataset(INPUT_DIM, VAL_STEPS_PER_EPOCH, BATCH_SIZE)\n",
    "        X_test = torch.randn(N_TEST_SAMPLES, INPUT_DIM, device=device) * WGM_NOISE_STD\n",
    "        test_dataset = TensorDataset(X_test)\n",
    "\n",
    "        #train_loader = DataLoader(train_dataset, batch_size=None, num_workers=0)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=None, num_workers=0)\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
    "        for method in methods_to_run:\n",
    "            print(f\"\\n--- Training method: {method} ---\")\n",
    "            estimator_mlp = GradientMLP2(INPUT_DIM, N_LAYERS, HIDDEN_DIM).to(device)\n",
    "            hparams_dict = {\n",
    "                \"input_dim\": INPUT_DIM, \"hidden_dim\": HIDDEN_DIM, \"n_layers\": N_LAYERS,\n",
    "                \"lr\": LEARNING_RATE, \"method\": method, \"wgm_lambda\": WGM_LAMBDA,\n",
    "                \"wgm_noise_std\": WGM_NOISE_STD, \"wgm_n_samples\": WGM_N_SAMPLES,\n",
    "                \"rfd_epsilon\": RFD_EPSILON, \"target_function\": target.mode,\n",
    "                \"problem_idx\": i, \"epochs\": MAX_EPOCHS, \"batch_size\": BATCH_SIZE,\n",
    "            }\n",
    "            experiment = GradientEstimationExperiment(target, estimator_mlp, method, LEARNING_RATE, hparams_dict)\n",
    "            logger = pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=f\"problem_{i}\", version=f\"{method}\")\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=MAX_EPOCHS, accelerator=\"auto\", devices=1, logger=logger,\n",
    "                enable_checkpointing=False, enable_progress_bar=True,\n",
    "                enable_model_summary=False, # Quieter output\n",
    "                num_sanity_val_steps=0,\n",
    "                inference_mode=False, # Important for grad calculations in val/test if needed by method\n",
    "                log_every_n_steps=TRAIN_STEPS_PER_EPOCH // 5,\n",
    "            )\n",
    "\n",
    "            print(f\"    Starting training for {method}...\")\n",
    "            trainer.fit(experiment, train_loader, val_loader)\n",
    "            print(f\"    Finished training for {method}.\")\n",
    "\n",
    "            print(f\"    Starting testing for {method}...\")\n",
    "            test_results = trainer.test(experiment, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "            if test_results:\n",
    "                final_mse = test_results[0].get('test_mse')\n",
    "                if final_mse is not None:\n",
    "                    all_results[method].append(final_mse)\n",
    "                    print(f\"    Method: {method}, Final Test MSE (Gradient): {final_mse:.6f}\")\n",
    "                else:\n",
    "                    all_results[method].append(float('nan'))\n",
    "            else:\n",
    "                all_results[method].append(float('nan'))\n",
    "\n",
    "            # --- STORE THE TRAINED MODEL ---\n",
    "            # Move to CPU and set to eval mode before storing\n",
    "            trained_estimators[method].append(experiment.estimator_mlp.cpu().eval())\n",
    "            # --------------------------------\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    print(f\"\\n{'='*10} Final Results Summary {'='*10}\")\n",
    "    for method in methods_to_run:\n",
    "        results = [res for res in all_results[method] if not np.isnan(res)]\n",
    "        if results:\n",
    "            avg_mse = np.mean(results) # Average over N_PROBLEMS (here, just 1)\n",
    "            std_mse = np.std(results)\n",
    "            print(f\"Method: {method:<12} | Avg Test MSE (Gradient): {avg_mse:.6f} +/- {std_mse:.6f}\")\n",
    "        else:\n",
    "            print(f\"Method: {method:<12} | No successful runs.\")\n",
    "\n",
    "    # --- IMPORTANT ---\n",
    "    # The 'trained_estimators' dictionary and the 'target' object\n",
    "    # now exist in your environment and can be used by the next block.\n",
    "    print(\"\\nTraining complete. Models stored in 'trained_estimators' dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2cf83-7ac4-4339-be58-7f1e8cb47456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= VISUALIZATION SCRIPT ======================\n",
    "# Make sure this part is run AFTER the main training script above,\n",
    "# so that 'target' and 'trained_estimators' are defined.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# La ligne suivante n'est plus strictement nécessaire avec les versions récentes\n",
    "# de matplotlib, mais elle était classiquement utilisée pour activer les outils 3D.\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "print(\"\\n{'='*10} Generating Visualizations {'='*10}\")\n",
    "\n",
    "# --- 1. Définir la grille de visualisation ---\n",
    "# Ajustez les limites si nécessaire en fonction de la fonction cible et de WGM_NOISE_STD\n",
    "# Par exemple, si WGM_NOISE_STD est petit, vous voudrez peut-être des limites plus serrées.\n",
    "# Pour 'styblinski', les minima intéressants sont autour de +/-2.9.\n",
    "# Pour 'ackley', le minimum est à (0,0) et la fonction est complexe.\n",
    "# Un range de [-5, 5] ou [-7, 7] est souvent un bon début pour ces fonctions.\n",
    "# Le code original utilisait [-10,10], ce qui est bien pour voir le comportement global.\n",
    "\n",
    "lim_plot = 3 # Ou 10, ou 5, dépendant de la fonction\n",
    "x_vis = np.linspace(-lim_plot, lim_plot, 100)\n",
    "y_vis = np.linspace(-lim_plot, lim_plot, 100)\n",
    "X_vis, Y_vis = np.meshgrid(x_vis, y_vis)\n",
    "\n",
    "# Préparer les entrées pour les modèles PyTorch\n",
    "# Les modèles s'attendent à une entrée de forme (batch_size, input_dim)\n",
    "xy_flat = np.stack([X_vis.ravel(), Y_vis.ravel()], axis=-1)\n",
    "xy_tensor = torch.tensor(xy_flat, dtype=torch.float32).cpu() # Mettre sur CPU\n",
    "\n",
    "# --- Fonction utilitaire pour tracer ---\n",
    "def plot_3d_surface_custom(X, Y, Z, title_str, fig_ax_pair, cmap_choice=plt.cm.viridis, z_offset_contours=-1.0):\n",
    "    \"\"\"\n",
    "    Fonction pour tracer une surface 3D avec maillage et contours.\n",
    "    fig_ax_pair: tuple (fig, ax)\n",
    "    \"\"\"\n",
    "    fig, ax = fig_ax_pair\n",
    "\n",
    "    # --- 3. Dessiner la surface ---\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=cmap_choice,\n",
    "                           linewidth=0.5, edgecolor='k', # Maillage noir fin\n",
    "                           antialiased=True)\n",
    "\n",
    "    # --- 4. Ajouter les lignes de niveau (contours) ---\n",
    "    num_niveaux = 15\n",
    "    min_Z_val = np.min(Z)\n",
    "    max_Z_val = np.max(Z)\n",
    "    # S'assurer que l'offset est raisonnable\n",
    "    contour_offset = min_Z_val + z_offset_contours if (max_Z_val - min_Z_val) > 1e-6 else min_Z_val - 0.5\n",
    "\n",
    "    ax.contour(X, Y, Z, zdir='z', offset=contour_offset,\n",
    "               levels=num_niveaux, cmap=cmap_choice,\n",
    "               linewidths=1)\n",
    "\n",
    "    # --- 5. Personnaliser le graphique ---\n",
    "    ax.set_xlabel('Axe X')\n",
    "    ax.set_ylabel('Axe Y')\n",
    "    ax.set_zlabel('Axe Z')\n",
    "    ax.set_title(title_str)\n",
    "\n",
    "    # Ajuster les limites de l'axe Z\n",
    "    z_lim_min = contour_offset - abs(0.5 * z_offset_contours) # Un peu d'espace sous les contours\n",
    "    z_lim_max = max_Z_val * 1.1 if max_Z_val >= 0 else max_Z_val * 0.9\n",
    "    if abs(z_lim_max - z_lim_min) < 1e-6: # Si la surface est plate\n",
    "        z_lim_min -= 0.5\n",
    "        z_lim_max += 0.5\n",
    "    ax.set_zlim(z_lim_min, z_lim_max)\n",
    "\n",
    "\n",
    "    # Ajouter une barre de couleur\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label='Valeur Z')\n",
    "\n",
    "    # Ajuster l'angle de vue\n",
    "    ax.view_init(elev=30., azim=45)#-60)\n",
    "    return surf\n",
    "\n",
    "\n",
    "# --- 2. Calculer et afficher la fonction CIBLE ---\n",
    "# Assurez-vous que 'target' est sur CPU et en mode eval\n",
    "target_cpu = target.cpu().eval()\n",
    "with torch.no_grad():\n",
    "    Z_target_flat = target_cpu(xy_tensor).numpy()\n",
    "Z_target = Z_target_flat.reshape(X_vis.shape)\n",
    "\n",
    "fig_target = plt.figure(figsize=(10, 8))\n",
    "ax_target = fig_target.add_subplot(111, projection='3d')\n",
    "plot_3d_surface_custom(X_vis, Y_vis, Z_target,\n",
    "                         f\"Fonction Cible: {target_cpu.mode.capitalize()}\",\n",
    "                         (fig_target, ax_target),\n",
    "                         cmap_choice=plt.cm.viridis) # Viridis pour la cible\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'surface_3d_target_{target_cpu.mode}.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 3. Calculer et afficher les ESTIMATEURS ---\n",
    "# (Rappel : N_PROBLEMS est à 1 dans votre config, donc on prend l'indice 0)\n",
    "problem_idx_to_plot = 0 # Puisque N_PROBLEMS = 1\n",
    "\n",
    "for method in methods_to_run[:1]:\n",
    "    if trained_estimators[method] and len(trained_estimators[method]) > problem_idx_to_plot:\n",
    "        estimator_model = trained_estimators[method][problem_idx_to_plot]\n",
    "        # Le modèle devrait déjà être sur CPU et en mode eval grâce à la sauvegarde\n",
    "        estimator_model.cpu().eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Z_estimator_flat = estimator_model(xy_tensor).numpy()\n",
    "        Z_estimator = Z_estimator_flat.reshape(X_vis.shape)\n",
    "\n",
    "        fig_est = plt.figure(figsize=(10, 8))\n",
    "        ax_est = fig_est.add_subplot(111, projection='3d')\n",
    "        plot_3d_surface_custom(X_vis, Y_vis, Z_estimator,\n",
    "                                 f\"Estimateur ({method.upper()}) pour {target_cpu.mode.capitalize()}\",\n",
    "                                 (fig_est, ax_est),\n",
    "                                 cmap_choice=plt.cm.plasma) # Plasma ou autre pour différencier\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'surface_3d_estimator_{method}_{target_cpu.mode}.png', dpi=50)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Pas de modèle entraîné trouvé pour la méthode '{method}' pour le problème {problem_idx_to_plot + 1} à visualiser.\")\n",
    "\n",
    "print(f\"{'='*10} Visualizations Complete {'='*10}\")\n",
    "\n",
    "# Optionnel: si vous voulez afficher Cible et Estimateurs sur la même figure avec des subplots\n",
    "# (Ceci est plus complexe si les échelles Z sont très différentes)\n",
    "\n",
    "# num_plots = 1 + len(methods_to_run)\n",
    "# cols = 2\n",
    "# rows = (num_plots + cols - 1) // cols\n",
    "\n",
    "# fig_all = plt.figure(figsize=(7 * cols, 6 * rows))\n",
    "# plot_idx = 1\n",
    "\n",
    "# # Plot Target\n",
    "# ax_all_target = fig_all.add_subplot(rows, cols, plot_idx, projection='3d')\n",
    "# plot_3d_surface_custom(X_vis, Y_vis, Z_target,\n",
    "#                          f\"Cible: {target_cpu.mode.capitalize()}\",\n",
    "#                          (fig_all, ax_all_target),\n",
    "#                          cmap_choice=plt.cm.viridis)\n",
    "# plot_idx += 1\n",
    "\n",
    "# # Plot Estimators\n",
    "# for method in methods_to_run:\n",
    "#     if trained_estimators[method] and len(trained_estimators[method]) > problem_idx_to_plot:\n",
    "#         estimator_model = trained_estimators[method][problem_idx_to_plot].cpu().eval()\n",
    "#         with torch.no_grad():\n",
    "#             Z_estimator_flat = estimator_model(xy_tensor).numpy()\n",
    "#         Z_estimator = Z_estimator_flat.reshape(X_vis.shape)\n",
    "\n",
    "#         ax_all_est = fig_all.add_subplot(rows, cols, plot_idx, projection='3d')\n",
    "#         plot_3d_surface_custom(X_vis, Y_vis, Z_estimator,\n",
    "#                                  f\"Estim. ({method.upper()})\", # Titre plus court pour subplot\n",
    "#                                  (fig_all, ax_all_est),\n",
    "#                                  cmap_choice=plt.cm.plasma)\n",
    "#         plot_idx += 1\n",
    "\n",
    "# plt.tight_layout(pad=3.0) # Ajouter du padding pour éviter chevauchement des titres/labels\n",
    "# plt.savefig(f'surface_3d_comparison_{target_cpu.mode}.png', dpi=300)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
